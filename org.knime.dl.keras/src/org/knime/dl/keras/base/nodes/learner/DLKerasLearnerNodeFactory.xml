<?xml version="1.0" encoding="UTF-8"?>
<knimeNode icon="dlkeraslearner.png" type="Learner" xmlns="http://knime.org/node/v3.6" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://knime.org/node/v3.6 http://knime.org/node/v3.6.xsd">
	<name>Keras Network Learner</name>

	<shortDescription>
		Performs supervised training on a Keras deep learning
		network.
	</shortDescription>

	<fullDescription>
		<intro>
			This node performs supervised learning on a Keras deep learning
			network.
		</intro>
		<tab name="General Settings">
			<option name="Back end">
				The deep learning back end which is used to
				train
				the input network.
			</option>
			<option name="Epochs">
				The number of iterations over the input training
				data.
			</option>
			<option name="Training batch size">
				The number of training data rows that are used for a single
				gradient update during training.
			</option>
			<option name="Validation batch size">
				The number of validation data rows that are processed at a time during validation.
				This option is only enabled if the node's validation data input port is connected.
			</option>
			<option name="Shuffle training data before each epoch">
				Shuffling the training data often improves the learning process because
				updating the network with the same batches in the same order in each epoch can have an detrimental
				effect on the convergence speed of the training.
			</option>
			<option name="Use random seed">
				If the checkbox is selected, the random seed displayed in the field on the right is used to perform the shuffling
				of the training data. Clicking the "New seed" button generates a new random seed.
				Leaving the checkbox unselected corresponds to creating a new seed for each execution of the node.
				NOTE: If your network contains weights that are initialized randomly, we currently don't seed this initialization.
				This means that you will very likely receive slightly different results for multiple model runs even though you are
				using the random seed for the shuffling of the training data.
			</option>
		</tab>
		<tab name="Optimizer Settings">
			<option name="Optimizer">
				The optimization algorithm. The following optimizers are available:
				<ul>
					<li>
						<a href="https://keras.io/optimizers/#adadelta">Adadelta</a>
					</li>
					<li>
						<a href="https://keras.io/optimizers/#adagrad">Adagrad</a>
					</li>
					<li>
						<a href="https://keras.io/optimizers/#adam">Adam</a>
					</li>
					<li>
						<a href="https://keras.io/optimizers/#adamax">Adamax</a>
					</li>
					<li>
						<a href="https://keras.io/optimizers/#nadam">Nadam</a>
					</li>
					<li>
						<a
							href="https://keras.io/optimizers/#rmsprop">RMSProp</a>
					</li>
					<li>
						<a href="https://keras.io/optimizers/#sgd">Stochastic gradient descent</a>
					</li>
				</ul>
				Please refer to the <a href="https://keras.io/optimizers/">Keras documentation</a> for further information on
				parameterization.
			</option>
			<option name="Clip norm">
				If checked, gradients whose L2 norm exceeds the
				given norm will be clipped to that norm.
			</option>
			<option name="Clip value">
				If checked, gradients whose absolute value
				exceeds the given value will be clipped to that value (or the
				negated value, respectively).
			</option>
		</tab>
		<tab name="Learning Behavior">
			<option name="Terminate on NaN loss">
				If checked, training is terminated if a NaN (not a number) training loss is encountered.
				Corresponds to the
				<a href="https://keras.io/callbacks/#terminateonnan">TerminateOnNaN Keras callback</a>.
			</option>
			<option name="Terminate on training stagnation (early stopping)">
				If checked, training is terminated if the monitored quantity has stopped improving.
				<ul>
					<li>Monitored quantity: the quantity on which early stopping is evaluated.
					Validation quantities are available for selection if the node's validation data input port is connected.</li>
					<li>Min. delta: minimum change of the monitored quantity which qualifies as an improvement.
					Absolute changes below this value are considered a stagnation.</li>
					<li>Patience: number of epochs with no improvements after which training will be stopped.</li>
				</ul>
				Corresponds to the
				<a href="https://keras.io/callbacks/#earlystopping">EarlyStopping Keras callback</a>.
			</option>
			<option name="Reduce learning rate on training stagnation">
				If checked, the learning rate is reduced if the monitored quantity has stopped improving.
				<ul>
					<li>Monitored quantity: the quantity on which learning rate reduction is evaluated.
					Validation quantities are available for selection if the node's validation data input port is connected.</li>
					<li>Factor: factor by which the learning rate will be reduced</li>
					<li>Patience: number of epochs with no improvements after which the learning rate will be reduced.</li>
					<li>Epsilon: threshold for measuring the new optimum, to only focus on significant changes.</li>
					<li>Cooldown: number of epochs to wait before resuming normal operation after the learning rate has been reduced.</li>
					<li>Min. learning rate: lower bound of the learning rate. The learning rate is not reduced below this value.</li>
				</ul>
				Corresponds to the
				<a href="https://keras.io/callbacks/#reducelronplateau">ReduceLROnPlateau Keras callback</a>.
			</option>
		</tab>
		<tab name="Input Data">
			<option name="Conversion">
				The converter that is used to transform the
				selected input columns into a format that is accepted by the
				respective network input specification.
			</option>
			<option name="Input columns">
				The table columns that are part of the respective
				network input.
				The availability of a column depends on the currently
				selected input converter.
			</option>
		</tab>
		<tab name="Target Data">
			<option name="Conversion">
				The converter that is used to transform the
				selected target columns into a format that is accepted by the
				respective network target specification.
			</option>
			<option name="Target columns">
				The table columns that are part of the respective
				network target.
				The availability of a column depends on the currently
				selected input converter.
			</option>
			<option name="Standard loss function">
				Choose one of the loss functions provided by Keras for your target.
			</option>
			<option name="Custom loss function">
				Define your own loss function as a Python snippet.
				The function <i>custom_loss</i> will be used as loss function for the target.
				It must always be present, therefore its signature is not editable.
			</option>
		</tab>
		<link href="https://www.knime.com/deeplearning/keras">
			KNIME Deep Learning Keras Integration
		</link>
	</fullDescription>

	<ports>
		<inPort index="0" name="Deep Learning Network">The input Keras deep learning network.
		</inPort>
		<inPort index="1" name="Training Data Table">The training data table that contains training
			and target columns.</inPort>
		<inPort index="2" name="Validation Data Table">The validation data table (optional).
			Must have the same column names and types in the same order as the training data table.
		</inPort>
		<outPort index="0" name="Deep Learning Network">The trained output Keras deep learning
			network.
		</outPort>
	</ports>

	<views>
		<view index="0" name="Learning Monitor">
			Shows information about the current learning run. Has an option for early
			stopping of training. If training
			is stopped before it is finished, the model will be saved in the current
			status.
		</view>
	</views>  
</knimeNode>
